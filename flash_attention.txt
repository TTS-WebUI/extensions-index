# Auto-generated from mjun0812/flash-attention-prebuild-wheels
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.9/flash_attn-2.6.3%2Bcu130torch2.9-cp314-cp314-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.14'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.7.4%2Bcu128torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.7.4%2Bcu128torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.7.4%2Bcu128torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.7.4%2Bcu128torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.7.4%2Bcu128torch2.9-cp314-cp314-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.14'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.7.4%2Bcu130torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.7.4%2Bcu130torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.7.4%2Bcu130torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.7.4%2Bcu130torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.7.4%2Bcu130torch2.9-cp314-cp314-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.14'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.8.3%2Bcu128torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.8.3%2Bcu128torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.8.3%2Bcu128torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.8.3%2Bcu128torch2.9-cp314-cp314-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.14'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.8.3%2Bcu130torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.8.3%2Bcu130torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.8.3%2Bcu130torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.8.3%2Bcu130torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.8/flash_attn-2.8.3%2Bcu130torch2.9-cp314-cp314-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.14'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.7.4%2Bcu124torch2.5-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.7.4%2Bcu124torch2.5-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.7.4%2Bcu124torch2.5-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.7.4%2Bcu124torch2.9-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.7.4%2Bcu124torch2.9-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.7.4%2Bcu124torch2.9-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu124torch2.5-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu124torch2.5-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu124torch2.5-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu124torch2.9-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu124torch2.9-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu124torch2.9-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.5-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.5-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.5-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.6-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.6-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.6-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.7-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.7-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.7-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.9-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.9-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu128torch2.9-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu130torch2.9-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu130torch2.9-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.4/flash_attn-2.8.3%2Bcu130torch2.9-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu124torch2.5-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu124torch2.9-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu124torch2.9-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu124torch2.9-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.5-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.5-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.5-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.6-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.6-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.6-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.7-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.7-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.7-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.9-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.9-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu128torch2.9-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu130torch2.9-cp310-cp310-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu130torch2.9-cp311-cp311-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.6.3/flash_attn-2.6.3%2Bcu130torch2.9-cp312-cp312-linux_aarch64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu124torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu126torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu128torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu130torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu130torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.6.3%2Bcu130torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu124torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu126torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu130torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.7.4%2Bcu130torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu124torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu126torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu128torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu130torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu130torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3%2Bcu130torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.2/flash_attn-2.8.3%2Bcu130torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.22/flash_attn-2.8.1%2Bcu128torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.22/flash_attn-2.8.1%2Bcu128torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.22/flash_attn-2.8.1%2Bcu128torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.22/flash_attn-2.8.1%2Bcu128torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.22/flash_attn-2.8.1%2Bcu130torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.22/flash_attn-2.8.1%2Bcu130torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.22/flash_attn-2.8.1%2Bcu130torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.22/flash_attn-2.8.1%2Bcu130torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.21/flash_attn-2.7.4%2Bcu128torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.21/flash_attn-2.7.4%2Bcu128torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.21/flash_attn-2.7.4%2Bcu128torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.21/flash_attn-2.7.4%2Bcu128torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.21/flash_attn-2.7.4%2Bcu130torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.21/flash_attn-2.7.4%2Bcu130torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.21/flash_attn-2.7.4%2Bcu130torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.21/flash_attn-2.7.4%2Bcu130torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.7-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.7-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.8-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.8-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.9-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.9-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu124torch2.9-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.7-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.8-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.8-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.8-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.9-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.9-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.9-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.7.4.post1%2Bcu126torch2.9-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.5-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.5-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.5-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.6-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.6-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.6-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.6-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.7-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.7-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.8-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.8-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.8-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.8-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.9-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.9-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.9-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu124torch2.9-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.5-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.5-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.5-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.6-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.6-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.6-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.6-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.7-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.7-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.8-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.8-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.8-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.8-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.9-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.9-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.9-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu126torch2.9-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu130torch2.9-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.19/flash_attn-2.8.3%2Bcu130torch2.9-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.6.3%2Bcu130torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.6.3%2Bcu130torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.6.3%2Bcu130torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.6.3%2Bcu130torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.7.4.post1%2Bcu130torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.7.4.post1%2Bcu130torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.8.3%2Bcu130torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.8.3%2Bcu130torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.8.3%2Bcu130torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.18/flash_attn-2.8.3%2Bcu130torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.6.3%2Bcu126torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.6.3%2Bcu126torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.6.3%2Bcu126torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.6.3%2Bcu126torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.6.3%2Bcu128torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.6.3%2Bcu128torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.6.3%2Bcu128torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.6.3%2Bcu128torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.7.4.post1%2Bcu126torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.7.4.post1%2Bcu126torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.7.4.post1%2Bcu126torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.7.4.post1%2Bcu126torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3%2Bcu126torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3%2Bcu126torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3%2Bcu126torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3%2Bcu126torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3%2Bcu128torch2.9-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3%2Bcu128torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3%2Bcu128torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.17/flash_attn-2.8.3%2Bcu128torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.6.3%2Bcu124torch2.5-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.6.3%2Bcu124torch2.6-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.6.3%2Bcu124torch2.7-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.6.3%2Bcu124torch2.8-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.6.3%2Bcu126torch2.5-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.6.3%2Bcu126torch2.6-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.6.3%2Bcu126torch2.7-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.6.3%2Bcu126torch2.8-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.7.4.post1%2Bcu124torch2.7-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.7.4.post1%2Bcu124torch2.8-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.7.4.post1%2Bcu126torch2.7-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.7.4.post1%2Bcu126torch2.8-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.8.3%2Bcu124torch2.5-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.8.3%2Bcu124torch2.6-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.8.3%2Bcu124torch2.7-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.8.3%2Bcu124torch2.8-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.8.3%2Bcu126torch2.5-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.8.3%2Bcu126torch2.6-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.8.3%2Bcu126torch2.7-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.16/flash_attn-2.8.3%2Bcu126torch2.8-cp39-cp39-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.9'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.15/flash_attn-2.8.3%2Bcu126torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.15/flash_attn-2.8.3%2Bcu126torch2.9-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.15/flash_attn-2.8.3%2Bcu126torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.15/flash_attn-2.8.3%2Bcu126torch2.9-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.15/flash_attn-2.8.3%2Bcu126torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.15/flash_attn-2.8.3%2Bcu126torch2.9-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.15/flash_attn-2.8.3%2Bcu128torch2.9-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.15/flash_attn-2.8.3%2Bcu128torch2.9-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.15/flash_attn-2.8.3%2Bcu128torch2.9-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.2%2Bcu124torch2.6-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.2%2Bcu124torch2.7-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.2%2Bcu124torch2.8-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.2%2Bcu126torch2.7-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.2%2Bcu126torch2.8-cp313-cp313-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu124torch2.6-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu124torch2.7-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu124torch2.8-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu126torch2.6-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu126torch2.7-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu126torch2.8-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu128torch2.6-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu128torch2.7-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu128torch2.8-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu129torch2.6-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.12/flash_attn-2.8.3%2Bcu129torch2.8-cp313-cp313-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.13'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu124torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu126torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu129torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu129torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu129torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu129torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu129torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3%2Bcu129torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.7.4%2Bcu128torch2.7-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.7.4%2Bcu128torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.7.4%2Bcu128torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.7.4%2Bcu128torch2.8-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.7.4%2Bcu128torch2.8-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.7.4%2Bcu128torch2.8-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2%2Bcu128torch2.7-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2%2Bcu128torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2%2Bcu128torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2%2Bcu128torch2.8-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2%2Bcu128torch2.8-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.10/flash_attn-2.8.2%2Bcu128torch2.8-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.9/flash_attn-2.7.4%2Bcu128torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu124torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu129torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu129torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu129torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu129torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu129torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.18/flash_attn-2.7.4%2Bcu129torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu124torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu129torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu129torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu129torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu129torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu129torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.6.3%2Bcu129torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.7.4%2Bcu124torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu124torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu129torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu129torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu129torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu129torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu129torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.14/flash_attn-2.8.2%2Bcu129torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.13/flash_attn-2.8.1%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu124torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.12/flash_attn-2.8.0%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu128torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.11/flash_attn-2.8.0%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.10/flash_attn-2.7.4%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.10/flash_attn-2.7.4%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.10/flash_attn-2.7.4%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.4.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.4.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.4.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.4-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.4-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.4-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.5-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.5-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.5-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.6-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.6-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.6-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.7-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu124torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.4-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.4-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.4-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.5-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.5-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.5-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.6-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.6-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.6-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.7-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.5.9%2Bcu128torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.4-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.4-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.4-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.5-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.6-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.6-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.6-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.7-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu124torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.4-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.4-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.4-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.5-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.5-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.5-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.6-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.6-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.6-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.7-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.6.3%2Bcu128torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.7.4%2Bcu124torch2.4-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.7.4%2Bcu124torch2.4-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.7.4%2Bcu124torch2.5-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.7.4%2Bcu124torch2.5-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.7.4%2Bcu124torch2.5-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.7.4%2Bcu124torch2.6-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.7.4%2Bcu124torch2.6-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.7.4%2Bcu124torch2.7-cp310-cp310-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.7.4%2Bcu124torch2.7-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.9/flash_attn-2.7.4%2Bcu124torch2.7-cp312-cp312-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.3.1/flash_attn-2.6.3%2Bcu126torch2.6-cp311-cp311-win_amd64.whl ; sys_platform == 'win32' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.4.3%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.4.3%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.4.3%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.5.9%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.5.9%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.5.9%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.6.3%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.6.3%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.6.3%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.7.4%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.7.4%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.1/flash_attn-2.7.4%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.0/flash_attn-2.4.3%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.0/flash_attn-2.4.3%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.0/flash_attn-2.4.3%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.0/flash_attn-2.5.9%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.0/flash_attn-2.5.9%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.0/flash_attn-2.5.9%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.0/flash_attn-2.6.3%2Bcu128torch2.8-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.0/flash_attn-2.6.3%2Bcu128torch2.8-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.2.0/flash_attn-2.6.3%2Bcu128torch2.8-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.4.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.4.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.4.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.5.9%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.5.9%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.5.9%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.6.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.6.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.6.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.7.4%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.7.4%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.7.4%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.9/flash_attn-2.4.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.9/flash_attn-2.4.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.9/flash_attn-2.4.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.9/flash_attn-2.5.9%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.9/flash_attn-2.5.9%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.9/flash_attn-2.5.9%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.9/flash_attn-2.6.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.9/flash_attn-2.6.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.9/flash_attn-2.6.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu118torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.4.3%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu118torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.5.9%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu118torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.6.3%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu118torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.8/flash_attn-2.7.4.post1%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu118torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.4.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu118torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.5.9%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu118torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.6.3%2Bcu128torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu118torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.7-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.7-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.7/flash_attn-2.7.4.post1%2Bcu126torch2.7-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.4.3%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.5.9%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.6.3%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.6/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.6.3%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu124torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.5/flash_attn-2.7.4.post1%2Bcu126torch2.6-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.4/flash_attn-2.7.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.3/flash_attn-2.7.2.post1%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.4.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.5.6%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.2/flash_attn-2.7.0.post2%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-1.0.9%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.4.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.6%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.5.9%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.0-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.1-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.2-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.3-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.4-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.5-cp310-cp310-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.10'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.1/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.4.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.6%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.5.9%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu118torch2.0-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu118torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu118torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu118torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu118torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu118torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu118torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu118torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu118torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu118torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu121torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu121torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu121torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu121torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu121torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu121torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu121torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu121torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu121torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu124torch2.1-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu124torch2.2-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu124torch2.2-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu124torch2.3-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu124torch2.3-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu124torch2.4-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu124torch2.4-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu124torch2.5-cp311-cp311-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.11'
flash-attn @ https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.0.0/flash_attn-2.6.3%2Bcu124torch2.5-cp312-cp312-linux_x86_64.whl ; sys_platform == 'linux' and python_version == '3.12'
